---
title: "Supplementary materials to the paper “Machine learning prediction models for donor Hb deferral: can we use the same models across countries?”"
authors: Amber Meulenbeld, Jarkko Toivonen, Tinus Brits, Ronel Swanevelder, Dorien de Clippel, Veerle Compernolle, Surendra Karki, Marijke Welvaert, Katja van den Hurk, Joost van Rosmalen, Emmanuel Lesaffre, Mart Janssen and Mikko Arvas
bibliography: bibliography.bib
csl: vox-sanguinis.csl
format:
  html:
    code-fold: true
#    page-layout: full
    toc: true
    embed-resources: true
---

# R Initialization and defining general Functions

```{r warning=F, message=F}
#| label: packages
#| code-summary: "Loading libraries"

library(stats)
library(randomForest)
library(ROCR)
library(kableExtra)
library(formatdown)

# Functions

# function to transform a numeric to a string with a fixed nr of decimals
fix_decimals<-function(x, digits=2, big_mark=NULL) gsub('\\$', '', format_decimal(x, digits=digits, big_mark = big_mark))

```

```{r}
#| label: create datasets function
#| code-summary: "function: createDatasets()"
createDatasets<-function(propdef=0, printresults=F){
  # function that creates three datasets: data_train, data_test, data_val
  # it returns the theoretical proportion of deferrals and the proportion of deferrals
  # in the training and test datasets (and all other settings)
  # PopMean is adapted to match propdef if this value is unequal to 0
  
  # set.seed(1)
  if(propdef!=0) { # set population mean to match the desired deferral rate
    pnorm(threshold, mean=PopMean, PopSd)
    fo<-function(x) (pnorm(threshold, mean=x, sqrt(PopSd^2+MeasSd^2))-propdef)^2
    PopMean <<- optimize(fo, c(0,2*PopMean))$minimum
    # print(threshold)
  }
  # simulate data
  true<-rnorm(n, mean=PopMean, sd=PopSd)
  meas1<-true+rnorm(n, mean=0, sd=MeasSd)
  dat<-as.data.frame(cbind(true, meas1))
  if (nr_meas>1) {
    for(i in 1:(nr_meas-1)) dat<-cbind(dat, true+rnorm(n, mean=0, MeasSd))
    dat$target<-(dat[,nr_meas+1]<threshold)*1
  } else { # only one measurement
    dat$target<-(dat$true<threshold)*1
  }
  dat$target<-as.factor(dat$target)
  colnames(dat)<-c("true", paste0("meas",1:nr_meas), "deferral")
  train<-sample(1:n, n*trainprop, replace=F)
  test<-sample((1:n)[!(1:n %in% train)], n*testprop, replace=F)
  val<-(1:n)[!(1:n %in% train) & !(1:n %in% test) ]
  data_train<<-dat[train,]
  data_test<<-dat[test,]
  data_val<<-dat[val,]
  if (printresults){
    print("Proportion deferrals")
    print(paste("theory:", pnorm(threshold, mean=PopMean, sqrt(PopSd^2+MeasSd^2))))
    print(paste("dataset overall:",sum(dat$deferral==1)/nrow(dat)))
    print(paste("ratio:",sum(dat$deferral==1)/nrow(dat)/pnorm(threshold, mean=PopMean, PopSd)))
    print("")
    print(paste("training  :", sum(data_train$deferral==1)/nrow(data_train)))
    print(paste("testing   :", sum(data_test$deferral==1)/nrow(data_test)))
    if(nrow(data_val)>0) print(paste("validation:", sum(data_val$deferral==1)/nrow(data_val)))
  } else {
    return(c(pnorm(threshold, mean=PopMean, sqrt(PopSd^2+MeasSd^2)), sum(data_train$deferral==1)/nrow(data_train),
      sum(data_test$deferral==1)/nrow(data_test) ))
  }
}
```

```{r}
#| label: create new test dataset function
#| code-summary: "function: createNewTestDataset()"
createNewTestDataset<-function(){
  # function that creates a new test dataset: data_test
  # note that it does not adapt PopMean to the propdef (as this takes time)
  # it returns the theoretical proportion of deferrals and the proportion of deferrals
  
  true<-rnorm(n*testprop, mean=PopMean, sd=PopSd)
  meas1<-true+rnorm(n*testprop, mean=0, sd=MeasSd)
  dat<-as.data.frame(cbind(true, meas1))
  if (nr_meas>1) {
    for(i in 1:(nr_meas-1)) dat<-cbind(dat, true+rnorm(n*testprop, mean=0, MeasSd))
    dat$target<-(dat[,nr_meas+1]<threshold)*1
  } else { # only one measurement
    dat$target<-(dat$true<threshold)*1
  }
  dat$target<-as.factor(dat$target)
  colnames(dat)<-c("true", paste0("meas",1:nr_meas), "deferral")
  data_test<<-dat
  # return proportion of deferrals
  #return(sum(data_test$deferral==1)/nrow(data_test))
}
```

```{r}
#| label: analyse training data function
#| code-summary: "function: analyseTrainingData()"
analyseTrainingData<-function(){
  # fit RF model
  RFmodel <<- randomForest(deferral~meas1, data=data_train)
  # calculate AUPR
  pred_train_p = predict(RFmodel, newdata = data_train, type = "prob")
  perf_train_data = prediction(pred_train_p[,2], data_train$deferral)
  train_aucpr = performance(perf_train_data, "aucpr")
  train_auc = performance(perf_train_data, "auc")
  return(c(train_aucpr@y.values[[1]],train_auc@y.values[[1]]))
}
```

```{r}
#| label: analyse test data function
#| code-summary: "function: analyseTestData()"

analyseTestData<-function(){
  # calculate AUPR
  pred_test_p = predict(RFmodel, newdata = data_test, type= "prob")
  perf_test_data = prediction(pred_test_p[,2], data_test$deferral)
  test_aucpr = performance(perf_test_data, "aucpr")
  test_auc = performance(perf_test_data, "auc")
  return(c(test_aucpr@y.values[[1]],test_auc@y.values[[1]]))
}

```

```{r}
#| label: simulation function
#| code-summary: "function: doSimulation()"
doSimulation<-function(nrsim=3){
  lp<-length(props)
  lm<-length(MeasSds)
  # define an array to store deferral rates per sample
  dsc <<-array(0,dim=c(nrsim,lp)) # training data
  dsc2<<-array(0,dim=c(nrsim,lp,lp,lm)) # test data
  # define array to store AUPR and ROC results per sample
  trdao <<-array(0,dim=c(nrsim,lp,2))  # training data
  tedao <<-array(0,dim=c(nrsim,lp,lp,lm,2)) # test data
  for (i in 1:nrsim){
    print(paste(i, "of", nrsim))
    for (j in 1:lp){ # for each level of deferral rate of donors create training dataset and model
      PopMean<<-PopMeans[j]
      MeasSd<<-MeasSds[1]
      d1 <- createDatasets(propdef=props[j])
      dsc[i,j]<<-sum(data_train$deferral==1)/nrow(data_train)
      trdao[i,j,]<<-analyseTrainingData()
      for (k in 1:lp){ # apply the trained model to a dataset for each level of deferral 
        # and level of  measurement variation
        for (l in 1:lm){
          PopMean<<-PopMeans[k,l]
          MeasSd<<-MeasSds[l]
          createNewTestDataset()
          dsc2 [i,j,k,l]<<-sum(data_test$deferral==1)/nrow(data_test)
          tedao[i,j,k,l,]<<-analyseTestData()
        }
      }
    }
  }
}
```

```{r}
#| label: plotting function
#| code-summary: "function: plotResultsorProportions()"

plotResultsorProportions <- function(res, auind, aucor, calcMedian, plotResults, plotProportions, plotExchanged, plotboth){
# function to plot simulation results

  # set spacing in x values for plotting
  d<<-min(props)/12
  
  # Extract statistics from the stored simulation output
  for (l in 1:lm){
    eval(parse(text=paste0("auMean",l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("auSd"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("auUL"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("auLL"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    
    eval(parse(text=paste0("dscMean",l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("dscSd"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("dscUL"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("dscLL"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    for (i in 1:lp){
      for (j in 1:lp){
        eval(parse(text=paste0("auMean",l,"[i,j]<<-mean(tedao    [,i,j,",l,",auind])")))
        if(calcMedian) eval(parse(text=paste0("auMean",l,"[i,j]<-median(tedao    [,i,j,",l,",auind])")))
        eval(parse(text=paste0("auSd"  ,l,"[i,j]<<-sd(tedao      [,i,j,",l,",auind])")))
        eval(parse(text=paste0("auUL"  ,l,"[i,j]<<-quantile(tedao[,i,j,",l,",auind],.975)")))
        eval(parse(text=paste0("auLL"  ,l,"[i,j]<<-max(0,quantile(tedao[,i,j,",l,",auind],.025))")))
        if (auind==1 & aucor==T){ # calculated corrected AUPR values
          eval(parse(text=paste0("auMean",l,"[i,j]<<-mean(tedao    [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
          if(calcMedian) eval(parse(text=paste0("auMean",l,"[i,j]<-median(tedao    [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
          eval(parse(text=paste0("auSd"  ,l,"[i,j]<<-sd(tedao      [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
          eval(parse(text=paste0("auUL"  ,l,"[i,j]<<-quantile(tedao[,i,j,",l,",auind]-dsc2[,i,j,",l,"],.975)")))
          eval(parse(text=paste0("auLL"  ,l,"[i,j]<<-max(0,quantile(tedao[,i,j,",l,",auind]-dsc2[,i,j,",l,"],.025))")))
        }
        
        eval(parse(text=paste0("dscMean",l,"[i,j]<<-mean(dsc2    [,i,j,",l,"])")))
        if(calcMedian) eval(parse(text=paste0("dscMean",l,"[i,j]<<-median(dsc2    [,i,j,",l,"])")))
        eval(parse(text=paste0("dscSd"  ,l,"[i,j]<<-sd(dsc2      [,i,j,",l,"])")))
        eval(parse(text=paste0("dscUL"  ,l,"[i,j]<<-quantile(dsc2[,i,j,",l,"],.975)")))
        eval(parse(text=paste0("dscLL"  ,l,"[i,j]<<-quantile(dsc2[,i,j,",l,"],.025)")))
      }
    }
  } 
  auMean<<-apply(trdao[,,auind],2,mean)
  if(calcMedian) auMean<<-apply(trdao[,,auind],2,median)
  auSd  <<-apply(trdao[,,auind],2,sd)
  auUL  <<-apply(trdao[,,auind],2,function (x) quantile(x,0.975))
  auLL  <<-apply(trdao[,,auind],2,function (x) quantile(x,0.025))
  if (auind==1 & aucor==T){ # calculated corrected AUPR values
    auMean<<-apply(trdao[,,auind]-dsc,2,mean)
    if(calcMedian) auMean<<-apply(trdao[,,auind]-dsc,2,median)
    auSd  <<-apply(trdao[,,auind]-dsc,2,sd)
    auUL  <<-apply(trdao[,,auind]-dsc,2,function (x) quantile(x,0.975))
    auLL  <<-apply(trdao[,,auind]-dsc,2,function (x) quantile(x,0.025))
  }   
  (dscMean<<-apply(dsc,2,mean))
  if(calcMedian) (dscMean<<-apply(dsc,2,median))
  dscUL   <<-apply(dsc,2,function (x) quantile(x,0.975))
  dscLL   <<-apply(dsc,2,function (x) quantile(x,0.025))
  
  #plotting proportions
  if(plotProportions){
    
    plot(c(props-d,props+d),c(dscMean,diag(dscMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.2), 
         ylab="Proportion deferrals in sample", xlab="Proportion deferrals")
    for (i in 1:lp) {
      lines(rep(props[i]-d,2),c(dscLL [i],   dscUL [i]))
      lines(rep(props[i]+d,2),c(dscLL1[i,i], dscUL1[i,i]))
    }
    legend("bottomright", c("Training data", "Test data", "95% Confidence interval"), lty=c(NA,NA,1), pch=c(1,4,NA))
    
  }
  if(plotResults){ #plotting results for own test data
    if(auind==1){
      if (aucor==T){ # corrected AUPR values
        plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.3),
             ylab="Corrected area under the PR curve", xlab="Proportion deferrals")
      } else { # corrected AUPR values
        plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.5),
             ylab="Area under the PR curve", xlab="Proportion deferrals")
      }
    } else {
      plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,1),
           ylab="Area under the ROC curve", xlab="Proportion deferrals")
  }
  for (i in 1:lp) {
    lines(rep(props[i]-d,2),c(auLL[i], auUL[i]))
    lines(rep(props[i]+d,2),c(auLL1[i,i], auUL1[i,i]))
  }
  legend("bottomright", c("Training data", "Test data", "95% Confidence interval"), lty=c(NA,NA,1), pch=c(1,4,NA))
  if(auind==2) abline(h=max(auMean), col=8, lty=2)}
  if(plotExchanged){ #plot the performance when exchanging the trained models across deferral rates
    cols<-c(1,2,3,4,6) #set colours of the different deferral rates
  if(auind==1) { # AUPR plots
    if (aucor==T){ # corrected AUPR values
      ylab="Corrected area under the PR curve"
      if(plotboth){
        ylim<-c(0,0.45)
      } else { # if plotted with alternative deferral probability dataset
        ylim<-c(0,0.3)
      }
    } else {
      ylab="Area under the PR curve"
      if(plotboth){
        ylim<-c(0,0.65)
      } else { # if plotted with alternative deferral probability dataset
        ylim<-c(0,0.5)
      }
    }
  } else { # AUROC plots
    ylab="Area under the AUROC curve"
    if(plotboth){
      ylim<-c(.5,1)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(.5,.9)
    }
  }
  plot(props-d,auMean, pch=rep(1,lp), ylim=ylim, xlim=c(0,0.17), col=cols,
       ylab=ylab, xlab="Proportion deferrals")
  # add training uncertainty
  for (i in 1:lp) lines(rep(props[i]-d,2),c(auLL[i], auUL[i]), col=cols[i])
  # add validation results and uncertainty estimates
  for (i in 1:lp) {
    lines(rep(props[i]+2*d,2), c(auLL1[1,i], auUL1[1,i]), col=cols[1])
    lines(rep(props[i]+3*d,2), c(auLL1[2,i], auUL1[2,i]), col=cols[2])
    lines(rep(props[i]+4*d,2), c(auLL1[3,i], auUL1[3,i]), col=cols[3])
    lines(rep(props[i]+5*d,2), c(auLL1[4,i], auUL1[4,i]), col=cols[4])
    lines(rep(props[i]+6*d,2), c(auLL1[5,i], auUL1[5,i]), col=cols[5])
    
    points(props[i]+2*d, auMean1[1,i], pch=4, col=cols[1])
    points(props[i]+3*d, auMean1[2,i], pch=4, col=cols[2])
    points(props[i]+4*d, auMean1[3,i], pch=4, col=cols[3])
    points(props[i]+5*d, auMean1[4,i], pch=4, col=cols[4])
    points(props[i]+6*d, auMean1[5,i], pch=4, col=cols[5])
    
    if (F) {
      points(props[i]+2*d, auUL1[1,i], pch=1)
      points(props[i]+3*d, auUL1[2,i], pch=1)
      points(props[i]+4*d, auUL1[3,i], pch=1)
      points(props[i]+5*d, auUL1[4,i], pch=1)
      points(props[i]+6*d, auUL1[5,i], pch=1)
      
      points(props[i]+2*d, auLL1[1,i], pch=1)
      points(props[i]+3*d, auLL1[2,i], pch=1)
      points(props[i]+4*d, auLL1[3,i], pch=1)
      points(props[i]+5*d, auLL1[4,i], pch=1)
      points(props[i]+6*d, auLL1[5,i], pch=1)
    }
  }
  
  # add legends
  legend(0.10,ylim[1]+.85*(ylim[2]-ylim[1]), paste0(props*100,"%"), title="proportion \ndeferrals\nin dataset",
         col=cols, lty=rep(1,lp), box.col=0, cex=0.75)
  if(!plotboth){
    legend(0.09, ylim[1]+.3*(ylim[2]-ylim[1]), c("Training data", "Test data (increasing deferral rates", 
                                                  "in training data from left to right)", "95% Confidence interval"), 
           lty=c(NA,NA,NA,1), pch=c(1,4,NA,NA), cex=0.75)
  }
  if (plotboth) { 
    d2=d/2
    auMean<-apply(trdao[,,2],2,mean)
    if(calcMedian) auMean<-apply(trdao[,,2],2,median)
    auSd  <-apply(trdao[,,2],2,sd)
    auLL  <-apply(trdao[,,2],2,function (x) quantile(x,0.975))
    auUL  <-apply(trdao[,,2],2,function (x) quantile(x,0.025))
    for (i in 1:lp) {
      lines(rep(props[i]+2*d+d2,2), c(auLL2[1,i], auUL2[1,i]), col=cols[1])
      lines(rep(props[i]+3*d+d2,2), c(auLL2[2,i], auUL2[2,i]), col=cols[2])
      lines(rep(props[i]+4*d+d2,2), c(auLL2[3,i], auUL2[3,i]), col=cols[3])
      lines(rep(props[i]+5*d+d2,2), c(auLL2[4,i], auUL2[4,i]), col=cols[4])
      lines(rep(props[i]+6*d+d2,2), c(auLL2[5,i], auUL2[5,i]), col=cols[5])
      
      points(props[i]+2*d+d2, auMean2[1,i], pch=2, col=cols[1])
      points(props[i]+3*d+d2, auMean2[2,i], pch=2, col=cols[2])
      points(props[i]+4*d+d2, auMean2[3,i], pch=2, col=cols[3])
      points(props[i]+5*d+d2, auMean2[4,i], pch=2, col=cols[4])
      points(props[i]+6*d+d2, auMean2[5,i], pch=2, col=cols[5])
    }
    legend(0.0875, ylim[1]+.4*(ylim[2]-ylim[1]), 
           c("Training data", "Test data 1 (increasing deferral rates", 
             "in training data from left to right)", 
             paste0("Sd of Measurements=", fix_decimals(MeasSds[1],2)),
             "Test data 2 ", paste0("Sd of Measurements=",fix_decimals(MeasSds[2],2)),
             "95% Confidence interval"), 
           lty=c(NA,NA,NA,NA,NA,NA,1), pch=c(1,4,NA,NA,2,NA,NA), cex=.7)
  }
  if(auind==2) {
    abline(h=max(auMean), col=8, lty=2)
  }
  }
}
```

# Setting various simulation parameters

```{r}
#| label: parameters
#| code-summary: "Parameters"

n<-1e4          # nr of simulation records
PopMean<-9.4    # population mean Hb
PopSd  <-0.7    # standard deviation of population Hb
MeasSd <-0.5    # standard deviation of Hb measurement
threshold<-8.4  # deferral threshold
nr_meas<-2      # number of measurements performed
propdef<-.03    # proportion deferrals to be analysed

# proportion deferrals to be analysed
props<-c(.01, .02, .04, .08, .16)
# measurement variability to be analysed
MeasSds<-c(0.5, 0.25)

# set train/test/validation proportions
trainprop<-0.80 # proportion training data
testprop <-0.20 # proportion test data

# set number of simulations
nrsimulations <- 300

# set seed value
set.seed(3)
```

# Description of a hypothetical donor population

```{r}
#| label: create dataset
#| results: false
#| code-summary: "Code population simulation"

(d1<-createDatasets(propdef=0.05, printresults = F))

p1<- pnorm(threshold, mean=PopMean, PopSd) # theoretical proportion of deferrals
p2<- pnorm(threshold, mean=PopMean, sqrt(PopSd^2+MeasSds[1]^2))
p3<- pnorm(threshold, mean=PopMean, sqrt(PopSd^2+MeasSds[2]^2))

createNewTestDataset()
```

To evaluate the behaviour of prediction models for donor deferral we simulated a hypothetical donor population with mean Hb level of `r fix_decimals(PopMean,1)` and a standard deviation of `r fix_decimals(PopSd,2)` mmol/L. We also presumed that the Hb level of donors would be measured with a measurement error having a standard deviation of `r fix_decimals(MeasSds[1],2)` mmol/L (without any bias).\
With a deferral threshold of `r threshold` this population should have `r fix_decimals(p1*100,1)`% of its donors deferred for a low Hb level. However, with a measurement uncertainty of `r fix_decimals(MeasSds[1],2)` mmol/L there will be on average a proportion of `r fix_decimals(p2*100,1)`% of measurements below the deferral threshold (see @fig-distr). In case the measurement variation would be only half as big (`r fix_decimals(MeasSds[2], 2)` mmol/L) there would on average only be a deferral rate of `r fix_decimals(p3*100,1)`%.

```{r}
#| label: fig-distr
#| fig-cap: Population Hb distribution and measured Hb distribution for different levels of measurement error (Sd of 0.50 and 0.25 mmol/L respectively).
#| code-summary: "Code plot densities"
#| cap-location: margin

  plot(density(data_train$true), lwd=2, main="", xlab="Hb level [mmol/L]")
  abline(v=threshold,col=8)
  dens<-density(data_train$meas1, bw=.13)
  lines(dens$x,dens$y, col="red", lwd=2)
  MeasSd<-MeasSds[2]
  createNewTestDataset()
  dens<-density(data_test$meas1, bw=.17)
  lines(dens$x,dens$y, col="green", lwd=2)
  MeasSd<-MeasSds[1]
  legend("topright", c("Population distribution",paste0("Measurement Sd=", fix_decimals(MeasSds[1],2)),paste0("Measurement Sd=", MeasSds[2]), "Deferral threshold"), 
         lty=c(1,1,1,1), col=c(1,2,3,8), lwd=c(2,2,2,1))

```

# Using a Random Forest model to predict donor deferral

We simulated a dataset of `r fix_decimals(n,digits=0, big_mark=",")` donations from the above population of which the Hb levels were measured with a standard deviation of `r fix_decimals(MeasSds[1],2)` mmol/L where we did two subsequent measurements. A random forest model was trained on `r trainprop*100`% of the data to predict donor deferral of the second Hb measurement using the Hb levels from the first measurement as a predictor. The quality of the deferral prediction model was expressed as the Corrected Area Under the Precision Recall curve (cAUPR) and as the area under the Receiver Operating Characteristic curve (AUROC) which were derived from applying the model to a test data set consisting of the remaining `r testprop*100`% of the data. The correction applied for the AUPR estimate consists of a subtraction of the overall deferral rate, as reported earlier @vinkenoog_international_2023. When comparing AUPR estimates from models applied in settings with different deferral rates these will be biased by the deferral rate as this is the baseline value for the AUPR estimate (at 100% recall). By subtracting the overall deferral rate a more objective reflection of what a model has actually learned will be obtained. @fig-AUPRcurve shows the (uncorrected) precision recall curve and ROC curve for a training and test dataset. @fig-AUROCurve shows the ROC curves for the same training and test datasets.

```{r}
#| label: fit model and calculate statistics
#| echo: true
#| results: false
#| code-summary: "Code RF model"

# Now fit a model
bestmtry <- tuneRF(data_train,data_train$deferral,stepFactor=1.20, improve = 0.010, trace=T, plot= F) 
RFmodel <- randomForest(deferral~meas1,data=data_train)
```

```{r}
#| label: performance 
#| code-summary: "Code performance statistics"

# Calculate various prediction characteristics 
pred_train_p = predict(RFmodel, type = "prob")
pred_train_p = predict(RFmodel, newdata=data_train, type = "prob")
perf_train_data = prediction(pred_train_p[,2], data_train$deferral)
train_perf = performance(perf_train_data, "prec","sens")
train_perf2 = performance(perf_train_data, "sens","spec")
train_perf2a = performance(perf_train_data, "tpr","fpr")
train_aucpr = performance(perf_train_data, "aucpr")
train_auc = performance(perf_train_data, "auc")

pred_test_p = predict(RFmodel, newdata = data_test, type= "prob")
perf_test_data = prediction(pred_test_p[,2], data_test$deferral)
test_perf = performance(perf_test_data, "prec","sens")
test_perf2 = performance(perf_test_data, "sens","spec")
test_perf2a = performance(perf_test_data, "tpr","fpr")
test_aucpr = performance(perf_test_data, "aucpr")
test_auc = performance(perf_test_data, "auc")
```

```{r}
library(xgboost)
library(data.table)

data_train <- setDT(data_train)
data_test <- setDT(data_test)

labels <- data_train$deferral
ts_label <- data_test$deferral

new_tr <- model.matrix(~.+0,data = data_train[,"meas1",with=F]) 
new_ts <- model.matrix(~.+0,data = data_test[,"meas1",with=F])

labels <- as.numeric(labels)-1
ts_label <- as.numeric(ts_label)-1

dtrain <- xgb.DMatrix(data = new_tr,label = labels) 
dtest <- xgb.DMatrix(data = new_ts,label=ts_label)

params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)

#xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)

xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100, print_every_n = 10, watchlist = c(train=dtrain),early_stopping_rounds = 10, maximize = F , eval_metric = "error")

xgbpred <- predict(xgb1,dtrain)
perf_train_data = prediction(xgbpred, data_train$deferral)
train_perf = performance(perf_train_data, "prec","sens")
train_perf2 = performance(perf_train_data, "sens","spec")
train_perf2a = performance(perf_train_data, "tpr","fpr")
train_aucpr = performance(perf_train_data, "aucpr")
train_auc = performance(perf_train_data, "auc")

train_aucpr@y.values[[1]]

xgbpred <- predict(xgb1,dtest)
perf_test_data = prediction(xgbpred, data_test$deferral)
test_perf = performance(perf_test_data, "prec","sens")
test_perf2 = performance(perf_test_data, "sens","spec")
test_perf2a = performance(perf_test_data, "tpr","fpr")
test_aucpr = performance(perf_test_data, "aucpr")
test_auc = performance(perf_test_data, "auc")

test_aucpr@y.values[[1]]

```


```{r}
#| label: fig-AUPRcurve
#| fig-cap-location: margin
#| fig-cap: Precision-recall curve of a Random Forrest deferral prediction model when applied to random training and test data sets with a 3% deferral rate.
#| code-summary: "Code plot PR curves"

plot(train_perf, main="Precision-recall curve for Random Forest",col=2,lwd=2, xlab="Recall")
lines(unlist(train_perf@x.values),unlist(train_perf@y.values), col=2, lwd=2)
lines(unlist(test_perf@x.values),unlist(test_perf@y.values), col=3, lwd=2)
legend("topright", c(paste0("Training data (AUPR=",round(train_aucpr@y.values[[1]],3),")"), paste0("Test data (AUPR=",round(test_aucpr@y.values[[1]],3),")")), col=c(2,3), lwd=c(2,2))
```

```{r}
#| label: fig-AUROCurve
#| fig-cap-location: margin
#| fig-cap: ROC of a Random Forrest deferral prediction model when applied to random training and test data sets with a 3% deferral rate.
#| code-summary: "Code plot ROC-Curve"

plot(train_perf2a, main="ROC-Curve for Random Forest",col=2,lwd=2)
lines(unlist(train_perf2a@x.values),unlist(train_perf2a@y.values), col=2, lwd=2)
lines(unlist(test_perf2a@x.values),unlist(test_perf2a@y.values), col=3, lwd=2)
legend("bottomright", c(paste0("Training data (AUC=",round(train_auc@y.values[[1]],3),")"), paste0("Test data (AUC=",round(test_auc@y.values[[1]],3),")")), col=c(2,3), lwd=c(2,2))
```

# Simulation with a replication of the RF fits

We repeated the RF fitting procedure `r nrsimulations` times for various deferral rates (`r substr(paste(paste0(props*100,"%,"), collapse =" "), 1, nchar(paste(paste0(props*100,"%,"), collapse =" "))-1)`) as shown in @fig-defrates. Deferral rates were set by adjusting the population mean Hb level accordingly. @fig-AUPR and @fig-AUROC show the cAUPR and AUROC estimates for RF models trained and tested at various deferral rates. These figures clearly show that both the cAUPR and AUROC estimates increase with an increase in donor deferral rate. Also, these figures show that the cAUPR is more sensitive to changes between the models tested at various deferral rates. This is because the AUPR is sensitive changes in the positive predictive value (or precision) of the model whereas the AUROC is not. This is the main reason that AUPR performance measures are recommended for evaluating models with unbalanced binary class outcomes @saito_precision-recall_2015. In @fig-AUPRexchange the AUPR estimates for RF models for various deferral rates are shown again, but now the results for models trained at all deferral rate levels applied to all deferral rate test data are shown as well. These results are provided in numerical form in @tbl-cAUPR. @fig-AUROCexchange and @tbl-AUROC provide the same information for AUROC estimates. From @fig-AUPRexchange and @fig-AUROCexchange it can be found that models trained on data sets with higher deferral rates generally have a (slightly) better performance than model trained on dataset with lower deferral rates. @fig-AUPRbothmeaserr and @fig-AUROCbothmeaserr show the result of applying these models to test datasets with a lower measurement error (`r MeasSds[2]` instead of `r fix_decimals(MeasSds[1],2)` mmol/L) but identical preset deferral rates. These figures clearly show the increase in performance of the prediction models when measurements are more accurate. The data presented in these figures is also provided in @tbl-cAUPR and @tbl-AUROC.

```{r}
#| label: analyse population means
#| results: false
#| code-summary: "Code determine population means for different deferral rates"

PopMeans<-matrix(NA, nrow=length(props), ncol=length(MeasSds))
#for all Sd levels considered
for (j in 1:length(MeasSds)) {
  print(paste("Setpoints for a measurement Sd of", round(MeasSds[j],2)))
  for (i in 1:length(props)){
    fo<-function(x) (pnorm(threshold, mean=x, sqrt(PopSd^2+MeasSds[j]^2))-props[i])^2
    sol<-optimize(fo, c(0,2*PopMean))
    PopMeans[i,j] <- sol$minimum
    # print  population mean, probability of population true level of deferral, 
    # and probability of measured deferrals
    print(paste(sol$minimum, pnorm(threshold, mean=PopMeans[i,j], PopSd),
                pnorm(threshold, mean=PopMeans[i,j], sqrt(PopSd^2+MeasSds[j]^2))))
  }
  print("")
}
```

```{r}
#| label: simulation
#| results: false
#| code-summary: "Code simulation"

if(!file.exists(paste0("SimresMatrix_",nrsimulations,"sim.rds"))){ # redo the simulation
  # do the simulation for n samples
  set.seed(1)
  doSimulation(nrsimulations)
  
  # write results to file
  res<- list(n=n, props=props, trainprop=trainprop, testprop=testprop, MeasSds=MeasSds, 
            PopSd=PopSd, threshold=threshold, nr_meas=nr_meas, 
            tedao=tedao, trdao=trdao, dsc=dsc, dsc2=dsc2)
  saveRDS(res, file=paste0("SimresMatrix_",nrsimulations,"sim.rds"))
} else {
  res<-readRDS(paste0("SimresMatrix_",nrsimulations,"sim.rds"))
}

dsc<-res$dsc
dsc2<-res$dsc2
tedao<-res$tedao
trdao<-res$trdao
props<-res$props
trainprop<-res$trainprop
testprop<-res$trainprop
MeasSds<-res$MeasSds
PopSd<-res$PopSd
threshold<-res$threshold
nr_meas<-res$nr_meas
# Calculate mean, sd, UL and LL AUPR/ROC values per combination of proportion of deferrals and measurement uncertainty
lp<-length(props)
lm<-length(MeasSds)
```

```{r}
#| label: fig-defrates
#| code-summary: "Code performance statistics and plot proportions"
#| fig-cap: The proportion of donors below the deferral threshold for the training and test data sets in the simulation as function of the preset proportion of deferral (1%, 2%, 4%, 8% and 16%).
#| fig-cap-location: margin

#auind: 1 for AUPR, 2 for ROC
#aucor: T for corrected area under the PR curve
#calcMedian: T for calculating median instead of mean
#plotResults: T for plotting AUPR/ROC results, F for plotting the deferral proportions of the samples
plotResultsorProportions(res, auind=1, aucor=T, calcMedian=F, plotResults =F, plotProportions=T, plotExchanged=F, plotboth=F)

```

## cAUPR Results

```{r}
#| label: fig-AUPR
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) over all simulations and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals.
#| fig-cap-location: margin
#| code-summary: "Code plot"

# plot AUPR results

plotResultsorProportions(res, auind=1, aucor=T, calcMedian=F, plotResults =T, plotProportions=F, plotExchanged=F, plotboth=F)

```

```{r}
#| label: fig-AUPRexchange
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets.
#| fig-cap-location: margin
#| code-summary: "Code plot"

# create plots
# plot main training results

plotResultsorProportions(res, auind=1, aucor=T, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=T, plotboth=F)
```

```{r}
#| label: tbl-cAUPR
#| code-summary: "Code table"
#| tbl-cap: "AUPR estimates and SD of Hb deferral prediction models developed on and applied to datasets with varying deferral rates and measurement errors."
#| tbl-cap-location: margin

aupr05 <- matrix(nrow=nrow(auMean1), ncol = ncol(auMean1)+ncol(auSd1))
for(i in 1:5){
  aupr05[1:5,(2*i)-1] <- auMean1[1:5,i] 
  aupr05[1:5,(2*i)] <- auSd1[1:5,i] 
}

aupr025 <- matrix(nrow=nrow(auMean2), ncol = ncol(auMean2)+ncol(auSd2))
for(i in 1:5){
  aupr025[,(2*i)-1] <- auMean2[,i] 
  aupr025[,(2*i)] <- auSd2[,i] 
}

rownames<- c("Training 1%", "Training 2%", "Training 4%", "Training 8%", "Training 16%")
colnames<- c("cAUPR", "Sd", "cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd")

auprtable <- rbind(aupr05, aupr025)
rownames(auprtable)<-c(rownames, rownames)
colnames(auprtable)<-colnames
kable(auprtable,
      digits=3,
      row.names=T
      )%>% add_header_above(c(" " = 1,"Test 1%"=2, "Test 2%"=2, "Test 4%"=2, "Test 8%"=2, "Test 16%"=2)) %>% pack_rows(index = c("Measurement error 0.50 mmol/L" = 5, "Measurement error 0.25 mmol/L" = 5))
```

```{r}
#| label: fig-AUPRbothmeaserr
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. In addition, cAUPR estimates for the prediction models are shown when applied to test datasets with lower measurement error (0.25 mmol/L).
#| fig-cap-location: margin
#| code-summary: "Code plot"
plotResultsorProportions(res, auind=1, aucor=T, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=T, plotboth=T)
```

## AUROC Results

```{r}
#| label: AUROC results
#| results: false
#| code-summary: "Code performance statistics (AUROC)"
plotResultsorProportions(res, auind=2, aucor=F, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=F, plotboth=F)

auroc05 <- matrix(nrow=nrow(auMean1), ncol = ncol(auMean1)+ncol(auSd1))
for(i in 1:5){
  auroc05[1:5,(2*i)-1] <- auMean1[1:5,i] 
  auroc05[1:5,(2*i)] <- auSd1[1:5,i] 
}
auroc025 <- matrix(nrow=nrow(auMean2), ncol = ncol(auMean2)+ncol(auSd2))
for(i in 1:5){
  auroc025[,(2*i)-1] <- auMean2[,i] 
  auroc025[,(2*i)] <- auSd2[,i] 
}
```

```{r}
#| label: fig-AUROC
#| fig-cap: Mean area under the ROC curve (AUROC) over all simulations and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| code-summary: "Code plot"
#| fig-cap-location: margin
plotResultsorProportions(res, auind=2, aucor=F, calcMedian=F, plotResults =T, plotProportions=F, plotExchanged=F, plotboth=F)
```

```{r}
#| label: fig-AUROCexchange
#| fig-cap: Mean area under the ROC curve (AUROC) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| code-summary: "Code plot"
#| fig-cap-location: margin

# create plots
# plot main training results
plotResultsorProportions(res, auind=2, aucor=F, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=T, plotboth=F)

```

```{r}
#| label: tbl-AUROC
#| code-summary: "Code table"
#| tbl-cap: "AUROC estimates and SD of Hb deferral prediction models developed on and applied to datasets with varying deferral rates and measurement errors."
#| tbl-cap-location: margin
rownames<- c("Training 1%", "Training 2%", "Training 4%", "Training 8%", "Training 16%")
colnames<- c("cAUPR", "Sd", "cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd")

auroctable <- rbind(auroc05, auroc025)
rownames(auroctable)<-c(rownames, rownames)
colnames(auroctable)<-colnames
kable(auroctable,
      digits=3,
      row.names=T
)%>% add_header_above(c(" " = 1,"Test 1%"=2, "Test 2%"=2, "Test 4%"=2, "Test 8%"=2, "Test 16%"=2)) %>% pack_rows(index = c("Measurement error 0.50 mmol/L" = 5, "Measurement error 0.25 mmol/L" = 5))
```

```{r}
#| label: fig-AUROCbothmeaserr
#| fig-cap: Mean area under the ROC curve (AUROC) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. In addition, AUROC estimates for the prediction models are shown when applied to test datasets with lower measurement error (0.25 mmol/L). For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| fig-cap-location: margin
#| code-summary: "Code plot"
plotResultsorProportions(res, auind=2, aucor=F, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=T, plotboth=T)

```
