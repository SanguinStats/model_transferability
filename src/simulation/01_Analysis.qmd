---
title: "Supplementary materials to the paper “Machine learning prediction models for donor Hb deferral: can we use the same models across countries?”"
author: Amber Meulenbeld, Jarkko Toivonen, Tinus Brits, Ronel Swanevelder, Dorien de Clippel, Veerle Compernolle, Surendra Karki, Marijke Welvaert, Katja van den Hurk, Joost van Rosmalen, Emmanuel Lesaffre, Mart Janssen and Mikko Arvas
bibliography: bibliography.bib
csl: vox-sanguinis.csl
format:
  html:
    code-fold: true
#    page-layout: full
    toc: true
    embed-resources: true
---

# Initialization

```{r warning=F, message=F}
#| label: packages
#| code-fold: false

library(stats)
library(randomForest)
library(ROCR)
library(kableExtra)
```

```{r setup, include=FALSE}
# Register an inline hook
knitr::knit_hooks$set(inline = function(x) {
  paste(custom_print(x), collapse = ", ")
})
# Define a function factory (from @eipi10 answer)
op <- function(d = 1) {
  function(x) sprintf(paste0("%1.", d, "f"), x)
}
# Use a closure
custom_print <- op()

```

# Functions
```{r} 
#| label: create datasets function
#| code-summary: "function: createDatasets()"
createDatasets<-function(propdef=0, printresults=F){
  # function that creates three datasets: data_train, data_test, data_val
  # it returns the theoretical proportion of deferrals and the proportion of deferrals
  # in the training and test datasets (and all other settings)
  # PopMean is adapted to match propdef if this value is unequal to 0
  
  # set.seed(1)
  if(propdef!=0) { # set population mean to match the desired deferral rate
    pnorm(threshold, mean=PopMean, PopSd)
    fo<-function(x) (pnorm(threshold, mean=x, sqrt(PopSd^2+MeasSd^2))-propdef)^2
    PopMean <<- optimize(fo, c(0,2*PopMean))$minimum
    # print(threshold)
  }
  # simulate data
  true<-rnorm(n, mean=PopMean, sd=PopSd)
  meas1<-true+rnorm(n, mean=0, sd=MeasSd)
  dat<-as.data.frame(cbind(true, meas1))
  if (nr_meas>1) {
    for(i in 1:(nr_meas-1)) dat<-cbind(dat, true+rnorm(n, mean=0, MeasSd))
    dat$target<-(dat[,nr_meas+1]<threshold)*1
  } else { # only one measurement
    dat$target<-(dat$true<threshold)*1
  }
  dat$target<-as.factor(dat$target)
  colnames(dat)<-c("true", paste0("meas",1:nr_meas), "deferral")
  train<-sample(1:n, n*trainprop, replace=F)
  test<-sample((1:n)[!(1:n %in% train)], n*testprop, replace=F)
  val<-(1:n)[!(1:n %in% train) & !(1:n %in% test) ]
  data_train<<-dat[train,]
  data_test<<-dat[test,]
  data_val<<-dat[val,]
  if (printresults){
    print("Proportion deferrals")
    print(paste("theory:", pnorm(threshold, mean=PopMean, sqrt(PopSd^2+MeasSd^2))))
    print(paste("dataset overall:",sum(dat$deferral==1)/nrow(dat)))
    print(paste("ratio:",sum(dat$deferral==1)/nrow(dat)/pnorm(threshold, mean=PopMean, PopSd)))
    print("")
    print(paste("training  :", sum(data_train$deferral==1)/nrow(data_train)))
    print(paste("testing   :", sum(data_test$deferral==1)/nrow(data_test)))
    if(nrow(data_val)>0) print(paste("validation:", sum(data_val$deferral==1)/nrow(data_val)))
  } else {
    return(c(pnorm(threshold, mean=PopMean, sqrt(PopSd^2+MeasSd^2)), sum(data_train$deferral==1)/nrow(data_train),
      sum(data_test$deferral==1)/nrow(data_test) ))
  }
}
```

```{r}
#| label: create new test dataset function
#| code-summary: "function: createNewTestDataset()"
createNewTestDataset<-function(){
  # function that creates a new test dataset: data_test
  # note that it does not adapt PopMean to the propdef (as this takes time)
  # it returns the theoretical proportion of deferrals and the proportion of deferrals
  
  true<-rnorm(n*testprop, mean=PopMean, sd=PopSd)
  meas1<-true+rnorm(n*testprop, mean=0, sd=MeasSd)
  dat<-as.data.frame(cbind(true, meas1))
  if (nr_meas>1) {
    for(i in 1:(nr_meas-1)) dat<-cbind(dat, true+rnorm(n*testprop, mean=0, MeasSd))
    dat$target<-(dat[,nr_meas+1]<threshold)*1
  } else { # only one measurement
    dat$target<-(dat$true<threshold)*1
  }
  dat$target<-as.factor(dat$target)
  colnames(dat)<-c("true", paste0("meas",1:nr_meas), "deferral")
  data_test<<-dat
  # return proportion of deferrals
  #return(sum(data_test$deferral==1)/nrow(data_test))
}
```

```{r}
#| label: analyse training data function
#| code-summary: "function: analyseTrainingData()"
analyseTrainingData<-function(){
  # fit RF model
  RFmodel <<- randomForest(deferral~meas1, data=data_train)
  # calculate AUPR
  pred_train_p = predict(RFmodel, type = "prob")
  perf_train_data = prediction(pred_train_p[,2], data_train$deferral)
  train_aucpr = performance(perf_train_data, "aucpr")
  train_auc = performance(perf_train_data, "auc")
  return(c(train_aucpr@y.values[[1]],train_auc@y.values[[1]]))
}
```

```{r}
#| label: analyse test data function
#| code-summary: "function: analyseTestData()"

analyseTestData<-function(){
  # calculate AUPR
  pred_test_p = predict(RFmodel, newdata = data_test, type= "prob")
  perf_test_data = prediction(pred_test_p[,2], data_test$deferral)
  test_aucpr = performance(perf_test_data, "aucpr")
  test_auc = performance(perf_test_data, "auc")
  return(c(test_aucpr@y.values[[1]],test_auc@y.values[[1]]))
}

```

```{r}
#| label: simulation function
#| code-summary: "function: dosim()"
dosim<-function(nrsim=3){
  lp<-length(props)
  lm<-length(MeasSds)
  # define an array to store deferral rates per sample
  dsc <<-array(0,dim=c(nrsim,lp)) # training data
  dsc2<<-array(0,dim=c(nrsim,lp,lp,lm)) # test data
  # define array to store AUPR and ROC results per sample
  trdao <<-array(0,dim=c(nrsim,lp,2))  # training data
  tedao <<-array(0,dim=c(nrsim,lp,lp,lm,2)) # test data
  for (i in 1:nrsim){
    print(paste(i, "of", nrsim))
    for (j in 1:lp){ # for each level of deferral rate of donors create training dataset and model
      PopMean<<-PopMeans[j]
      MeasSd<<-MeasSds[1]
      d1 <- createDatasets(propdef=props[j])
      dsc[i,j]<<-sum(data_train$deferral==1)/nrow(data_train)
      trdao[i,j,]<<-analyseTrainingData()
      for (k in 1:lp){ # apply the trained model to a dataset for each level of deferral 
        # and level of  measurement variation
        for (l in 1:lm){
          PopMean<<-PopMeans[k,l]
          MeasSd<<-MeasSds[l]
          createNewTestDataset()
          dsc2 [i,j,k,l]<<-sum(data_test$deferral==1)/nrow(data_test)
          tedao[i,j,k,l,]<<-analyseTestData()
        }
      }
    }
  }
}
```

```{r}
#| label: plotting function
#| code-summary: "function: plotResultsorProportions()"

plotResultsorProportions <- function(res, auind, aucor, calcMedian, plotResults, plotProportions, plotExchanged, plotboth){
  d<<-min(props)/12 #delta x value for plotting
  for (l in 1:lm){
    eval(parse(text=paste0("auMean",l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("auSd"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("auUL"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("auLL"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    
    eval(parse(text=paste0("dscMean",l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("dscSd"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("dscUL"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    eval(parse(text=paste0("dscLL"  ,l,"<<-matrix(0, nrow=lp, ncol=lp)")))
    for (i in 1:lp){
      for (j in 1:lp){
        eval(parse(text=paste0("auMean",l,"[i,j]<<-mean(tedao    [,i,j,",l,",auind])")))
        if(calcMedian) eval(parse(text=paste0("auMean",l,"[i,j]<-median(tedao    [,i,j,",l,",auind])")))
        eval(parse(text=paste0("auSd"  ,l,"[i,j]<<-sd(tedao      [,i,j,",l,",auind])")))
        eval(parse(text=paste0("auUL"  ,l,"[i,j]<<-quantile(tedao[,i,j,",l,",auind],.975)")))
        eval(parse(text=paste0("auLL"  ,l,"[i,j]<<-max(0,quantile(tedao[,i,j,",l,",auind],.025))")))
        if (auind==1 & aucor==T){ # calculated corrected AUPR values
          eval(parse(text=paste0("auMean",l,"[i,j]<<-mean(tedao    [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
          if(calcMedian) eval(parse(text=paste0("auMean",l,"[i,j]<-median(tedao    [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
          eval(parse(text=paste0("auSd"  ,l,"[i,j]<<-sd(tedao      [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
          eval(parse(text=paste0("auUL"  ,l,"[i,j]<<-quantile(tedao[,i,j,",l,",auind]-dsc2[,i,j,",l,"],.975)")))
          eval(parse(text=paste0("auLL"  ,l,"[i,j]<<-max(0,quantile(tedao[,i,j,",l,",auind]-dsc2[,i,j,",l,"],.025))")))
        }
        
        eval(parse(text=paste0("dscMean",l,"[i,j]<<-mean(dsc2    [,i,j,",l,"])")))
        if(calcMedian) eval(parse(text=paste0("dscMean",l,"[i,j]<<-median(dsc2    [,i,j,",l,"])")))
        eval(parse(text=paste0("dscSd"  ,l,"[i,j]<<-sd(dsc2      [,i,j,",l,"])")))
        eval(parse(text=paste0("dscUL"  ,l,"[i,j]<<-quantile(dsc2[,i,j,",l,"],.975)")))
        eval(parse(text=paste0("dscLL"  ,l,"[i,j]<<-quantile(dsc2[,i,j,",l,"],.025)")))
      }
    }
  } 
  auMean<<-apply(trdao[,,auind],2,mean)
  if(calcMedian) auMean<<-apply(trdao[,,auind],2,median)
  auSd  <<-apply(trdao[,,auind],2,sd)
  auUL  <<-apply(trdao[,,auind],2,function (x) quantile(x,0.975))
  auLL  <<-apply(trdao[,,auind],2,function (x) quantile(x,0.025))
  if (auind==1 & aucor==T){ # calculated corrected AUPR values
    auMean<<-apply(trdao[,,auind]-dsc,2,mean)
    if(calcMedian) auMean<<-apply(trdao[,,auind]-dsc,2,median)
    auSd  <<-apply(trdao[,,auind]-dsc,2,sd)
    auUL  <<-apply(trdao[,,auind]-dsc,2,function (x) quantile(x,0.975))
    auLL  <<-apply(trdao[,,auind]-dsc,2,function (x) quantile(x,0.025))
  }   
  (dscMean<<-apply(dsc,2,mean))
  if(calcMedian) (dscMean<<-apply(dsc,2,median))
  dscUL   <<-apply(dsc,2,function (x) quantile(x,0.975))
  dscLL   <<-apply(dsc,2,function (x) quantile(x,0.025))
  
  #plotting proportions
  if(plotProportions){
    
    plot(c(props-d,props+d),c(dscMean,diag(dscMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.2), 
         ylab="Proportion deferrals in sample", xlab="Proportion deferrals")
    for (i in 1:lp) {
      lines(rep(props[i]-d,2),c(dscLL [i],   dscUL [i]))
      lines(rep(props[i]+d,2),c(dscLL1[i,i], dscUL1[i,i]))
    }
    legend("bottomright", c("Training data", "Test data", "95% Confidence interval"), lty=c(NA,NA,1), pch=c(1,4,NA))
    
  }
  if(plotResults){ #plotting results for own test data
    if(auind==1){
    if (aucor==T){ # corrected AUPR values
      plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.3),
           ylab="Corrected area under the PR curve", xlab="Proportion deferrals")
    } else { # corrected AUPR values
      plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.5),
           ylab="Area under the PR curve", xlab="Proportion deferrals")
    }
  } else {
    plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,1),
         ylab="Area under the ROC curve", xlab="Proportion deferrals")
  }
  for (i in 1:lp) {
    lines(rep(props[i]-d,2),c(auLL[i], auUL[i]))
    lines(rep(props[i]+d,2),c(auLL1[i,i], auUL1[i,i]))
  }
  legend("bottomright", c("Training data", "Test data", "95% Confidence interval"), lty=c(NA,NA,1), pch=c(1,4,NA))
  if(auind==2) abline(h=max(auMean), col=8, lty=2)}
  if(plotExchanged){ #plot the performance when exchanging the trained models across deferral rates
    cols<-c(1,2,3,4,6) #set colours of the different deferral rates
  if(auind==1) { # AUPR plots
    if (aucor==T){ # corrected AUPR values
      ylab="Corrected area under the PR curve"
      if(plotboth){
        ylim<-c(0,0.45)
      } else { # if plotted with alternative deferral probability dataset
        ylim<-c(0,0.3)
      }
    } else {
      ylab="Area under the PR curve"
      if(plotboth){
        ylim<-c(0,0.65)
      } else { # if plotted with alternative deferral probability dataset
        ylim<-c(0,0.5)
      }
    }
  } else { # AUROC plots
    ylab="Area under the AUROC curve"
    if(plotboth){
      ylim<-c(.5,1)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(.5,.9)
    }
  }
  plot(props-d,auMean, pch=rep(1,lp), ylim=ylim, xlim=c(0,0.17), col=cols,
       ylab=ylab, xlab="Proportion deferrals")
  # add training uncertainty
  for (i in 1:lp) lines(rep(props[i]-d,2),c(auLL[i], auUL[i]), col=cols[i])
  # add validation results and uncertainty estimates
  for (i in 1:lp) {
    lines(rep(props[i]+2*d,2), c(auLL1[1,i], auUL1[1,i]), col=cols[1])
    lines(rep(props[i]+3*d,2), c(auLL1[2,i], auUL1[2,i]), col=cols[2])
    lines(rep(props[i]+4*d,2), c(auLL1[3,i], auUL1[3,i]), col=cols[3])
    lines(rep(props[i]+5*d,2), c(auLL1[4,i], auUL1[4,i]), col=cols[4])
    lines(rep(props[i]+6*d,2), c(auLL1[5,i], auUL1[5,i]), col=cols[5])
    
    points(props[i]+2*d, auMean1[1,i], pch=4, col=cols[1])
    points(props[i]+3*d, auMean1[2,i], pch=4, col=cols[2])
    points(props[i]+4*d, auMean1[3,i], pch=4, col=cols[3])
    points(props[i]+5*d, auMean1[4,i], pch=4, col=cols[4])
    points(props[i]+6*d, auMean1[5,i], pch=4, col=cols[5])
    
    if (F) {
      points(props[i]+2*d, auUL1[1,i], pch=1)
      points(props[i]+3*d, auUL1[2,i], pch=1)
      points(props[i]+4*d, auUL1[3,i], pch=1)
      points(props[i]+5*d, auUL1[4,i], pch=1)
      points(props[i]+6*d, auUL1[5,i], pch=1)
      
      points(props[i]+2*d, auLL1[1,i], pch=1)
      points(props[i]+3*d, auLL1[2,i], pch=1)
      points(props[i]+4*d, auLL1[3,i], pch=1)
      points(props[i]+5*d, auLL1[4,i], pch=1)
      points(props[i]+6*d, auLL1[5,i], pch=1)
    }
  }
  
  # add legends
  legend(0.11,ylim[1]+.85*(ylim[2]-ylim[1]), paste0(props*100,"%"), title="proportion \ndeferrals\nin dataset",
         col=cols, lty=rep(1,lp), box.col=0, cex=0.75)
  if(!plotboth){
    legend(0.095, ylim[1]+.3*(ylim[2]-ylim[1]), c("Training data", "Test data (increasing deferral rate", 
                                                  "training data from left to right)", "95% Confidence interval"), 
           lty=c(NA,NA,NA,1), pch=c(1,4,NA,NA), cex=0.75)
  }
  if (plotboth) { 
    d2=d/2
    auMean<-apply(trdao[,,2],2,mean)
    if(calcMedian) auMean<-apply(trdao[,,2],2,median)
    auSd  <-apply(trdao[,,2],2,sd)
    auLL  <-apply(trdao[,,2],2,function (x) quantile(x,0.975))
    auUL  <-apply(trdao[,,2],2,function (x) quantile(x,0.025))
    for (i in 1:lp) {
      lines(rep(props[i]+2*d+d2,2), c(auLL2[1,i], auUL2[1,i]), col=cols[1])
      lines(rep(props[i]+3*d+d2,2), c(auLL2[2,i], auUL2[2,i]), col=cols[2])
      lines(rep(props[i]+4*d+d2,2), c(auLL2[3,i], auUL2[3,i]), col=cols[3])
      lines(rep(props[i]+5*d+d2,2), c(auLL2[4,i], auUL2[4,i]), col=cols[4])
      lines(rep(props[i]+6*d+d2,2), c(auLL2[5,i], auUL2[5,i]), col=cols[5])
      
      points(props[i]+2*d+d2, auMean2[1,i], pch=2, col=cols[1])
      points(props[i]+3*d+d2, auMean2[2,i], pch=2, col=cols[2])
      points(props[i]+4*d+d2, auMean2[3,i], pch=2, col=cols[3])
      points(props[i]+5*d+d2, auMean2[4,i], pch=2, col=cols[4])
      points(props[i]+6*d+d2, auMean2[5,i], pch=2, col=cols[5])
    }
    legend(0.095, ylim[1]+.4*(ylim[2]-ylim[1]), c("Training data", "Test data 1 (increasing deferral rates", 
                                                  "in training data from left to right)", paste0("Sd of Measurements=",
                                                                                                 round(MeasSds[1],2)),  "Test data 2 ", paste0("Sd of Measurements=",
                                                                                                                                               round(MeasSds[2],2)),"95% Confidence interval"), cex=.9,
           lty=c(NA,NA,NA,NA,NA,NA,1), pch=c(1,4,NA,NA,2,NA,NA))
  }
  if(auind==2) {
    abline(h=max(auMean), col=8, lty=2)
  }
  }
}
```

# Parameters

```{r}
#| label: parameters
#| code-summary: "Parameters"

n<-1e4          # nr of simulation records
PopMean<-9.4    # population mean Hb
PopSd  <-0.7    # standard deviation of population Hb
MeasSd <-0.5    # standard deviation of Hb measurement
threshold<-8.4  # deferral threshold
nr_meas<-2      # number of measurements performed
propdef<-.05    # proportion deferrals to be analysed

# proportion deferrals to be analysed
props<-c(.01, .02, .04, .08, .16)
# measurement variability to be analysed
MeasSds<-c(0.5, 0.25)

# set train/test/validation proportions
trainprop<-0.80 # proportion training data
testprop <-0.20 # proportion test data

# set number of simulations
nrsimulations <- 300

# set seed value
set.seed(1)
```


# Data simulation

```{r}
#| label: create dataset
#| results: false
#| code-summary: "Code data simulation"

(d1<-createDatasets(propdef=0.03, printresults = F))

p1<- pnorm(threshold, mean=PopMean, PopSd) # theoretical proportion of deferrals
p2<- pnorm(threshold, mean=PopMean, sqrt(PopSd^2+MeasSds[1]^2))
p3<- pnorm(threshold, mean=PopMean, sqrt(PopSd^2+MeasSds[2]^2))

createNewTestDataset()


```
To evaluate the behaviour of prediction models for donor deferral we simulated an artificial donor population with mean Hb level of `r PopMean` and a Sd of `r PopSd` mmol/L. 
```{r}
#| label: two-decimals
#| include: false
custom_print <- op(d = 2)
```
We also presumed that the Hb level of donors would be measured with a measurement error with a standard deviation of `r MeasSds[1]` mmol/L (without any bias).

```{r}
#| label: fig-distr
#| fig-cap: Population Hb distribution and measured Hb distribution for different measurement levels of measurement error (Sd of 0.50 and 0.25 mmol/L respectively).
#| code-summary: "Code plot"
#| cap-location: margin

  plot(density(data_train$true), lwd=2, main="", xlab="Hb level [mmol/L]")
  abline(v=threshold,col=8)
  dens<-density(data_train$meas1, bw=.13)
  lines(dens$x,dens$y, col="red", lwd=2)
  MeasSd<-MeasSds[2]
  createNewTestDataset()
  dens<-density(data_test$meas1, bw=.17)
  lines(dens$x,dens$y, col="green", lwd=2)
  MeasSd<-MeasSds[1]
  legend("topright", c("Population distribution","Measurement Sd=0.50","Measurement Sd=0.25"), 
         lty=c(1,1,1), col=c(1,2,3), lwd=c(2,2,2))

```

This population has a deferral rate of `r p1*100`%. This will lead to an observed proportion of `r p2*100`% of measurements below the deferral threshold (see @fig-distr). In case the measurement variation would be only half as big (`r round(MeasSds[2], digits=2)` mmol/L) there would still be a deferral rate of `r p3*100`%.

# Model fitting

```{r no-decimals}
#| label: no-decimals
#| include: false
custom_print <- op(d = 0)
```

We simulated a dataset of `r nrow(data_test)+nrow(data_train)` donations from the above population of which the Hb levels were measured with a standard deviation of `r MeasSd`  mmol/L where we did two subsequent measurements. A random forest model was trained on `r nrow(data_train)/(nrow(data_test)+nrow(data_train))*100`% of the data to predict donor deferral on the second Hb measurement using the Hb levels from the first measurement as a predictor. The quality of the deferral prediction model was expressed as the Corrected Area Under the Precision Recall curve (cAUPR) and as the area under the Receiver Operating Characteristic curve (AUROC) which were derived from applying the model to a test data set consisting of the remaining `r nrow(data_test)/(nrow(data_test)+nrow(data_train))*100`% of the data. The correction applied for the AUPR estimates consists of a subtraction of the overall deferral rate, as reported earlier @vinkenoog_international_2023.

@fig-AUPRcurve shows the (uncorrected) precision recall curve and @fig-AUROCurve the ROC curve for a training and test dataset. 

```{r}
#| label: fit model and calculate statistics
#| echo: true
#| results: false
#| code-summary: "Code RF model"

# Now fit a model
bestmtry <- tuneRF(data_train,data_train$deferral,stepFactor=1.20, improve = 0.010, trace=T, plot= F) 
RFmodel <- randomForest(deferral~meas1,data=data_train)
```



```{r}
#| label: performance 
#| code-summary: "Code performance statistics"

# Calculate various prediction characteristics 
pred_train_p = predict(RFmodel, type = "prob")
perf_train_data = prediction(pred_train_p[,2], data_train$deferral)
train_perf = performance(perf_train_data, "prec","sens")
train_perf2 = performance(perf_train_data, "sens","spec")
train_perf2a = performance(perf_train_data, "tpr","fpr")
train_aucpr = performance(perf_train_data, "aucpr")
train_auc = performance(perf_train_data, "auc")

pred_test_p = predict(RFmodel, newdata = data_test, type= "prob")
perf_test_data = prediction(pred_test_p[,2], data_test$deferral)
test_perf = performance(perf_test_data, "prec","sens")
test_perf2 = performance(perf_test_data, "sens","spec")
test_perf2a = performance(perf_test_data, "tpr","fpr")
test_aucpr = performance(perf_test_data, "aucpr")
test_auc = performance(perf_test_data, "auc")
```


```{r}
#| label: fig-AUPRcurve
#| fig-cap-location: margin
#| fig-cap: Precision-recall curve of a Random Forrest deferral prediction model when applied to training and test data sets with a 3% deferral rate.
#| code-summary: "Code plot"

plot(train_perf, main="Precision-recall curve for Random Forest",col=2,lwd=2, xlab="Recall")
lines(unlist(train_perf@x.values),unlist(train_perf@y.values), col=2, lwd=2)
lines(unlist(test_perf@x.values),unlist(test_perf@y.values), col=3, lwd=2)
legend("topright", c(paste0("Training data (AUPR=",round(train_aucpr@y.values[[1]],3),")"), paste0("Test data (AUPR=",round(test_aucpr@y.values[[1]],3),")")), col=c(2,3), lwd=c(2,2))
```

```{r}
#| label: fig-AUROCurve
#| fig-cap-location: margin
#| fig-cap: ROC of a Random Forrest deferral prediction model when applied to training and test data sets with a 3% deferral rate.
#| code-summary: "Code plot"

plot(train_perf2a, main="ROC-Curve for Random Forest",col=2,lwd=2)
lines(unlist(train_perf2a@x.values),unlist(train_perf2a@y.values), col=2, lwd=2)
lines(unlist(test_perf2a@x.values),unlist(test_perf2a@y.values), col=3, lwd=2)
legend("bottomright", c(paste0("Training data (AUC=",round(train_auc@y.values[[1]],3),")"), paste0("Test data (AUC=",round(test_auc@y.values[[1]],3),")")), col=c(2,3), lwd=c(2,2))
```

# Results

We repeated the simulation procedure `r nrsimulations` times for various deferral rates (`r props*100`%) as shown in @fig-defrates. Deferral rates were set by adjusting the mean population Hb levels accordingly. @fig-AUPR and @fig-AUROC show the cAUPR and AUROC estimates for RF models trained and tested at various deferral rates. These figures clearly show that both the cAUPR and AUROC estimates increase with an increase in donor deferral rate. Also, these figures show that the cAUPR is more sensitive to changes between the models tested at various deferral rates. This is because the AUPR is sensitive changes in the positive predictive value (or precision) of the model whereas the AUROC is not. This is the main reason that AUPR performance measures are recommended for evaluating models with unbalanced binary class outcomes @saito_precision-recall_2015. 
```{r}
#| label: two-decimals2
#| include: false
custom_print <- op(d = 2)
```
In @fig-AUPRexchange the AUPR estimates for RF models for various deferral rates are shown again, but now the results for models trained at all deferral rate levels applied to all deferral rate test data are shown as well. These results are provided in numerical form in @tbl-cAUPR. @fig-AUROCexchange and @tbl-AUROC provide the same information for AUROC estimates. From @fig-AUPRexchange and @fig-AUROCexchange it can be found that models trained on data sets with higher deferral rates generally have a (slightly) better performance than model trained on dataset with lower deferral rates. @fig-AUPRbothmeaserr and @fig-AUROCbothmeaserr show the result of applying these models to test datasets with a lower measurement error (`r MeasSds[2]` instead of `r MeasSds[1]` mmol/L) but identical preset deferral rates. These figures clearly show the increase in performance of the prediction models when measurements are more accurate. The data presented in these figures is also provided in @tbl-cAUPR and @tbl-AUROC.


```{r}
#| label: analyse population means
#| results: false
#| code-summary: "Code determine population means for different deferral rates"

PopMeans<-matrix(NA, nrow=length(props), ncol=length(MeasSds))
#for all Sd levels considered
for (j in 1:length(MeasSds)) {
  print(paste("Setpoints for a measurement Sd of", round(MeasSds[j],2)))
  for (i in 1:length(props)){
    fo<-function(x) (pnorm(threshold, mean=x, sqrt(PopSd^2+MeasSds[j]^2))-props[i])^2
    sol<-optimize(fo, c(0,2*PopMean))
    PopMeans[i,j] <- sol$minimum
    # print  population mean, probability of population true level of deferral, 
    # and probability of measured deferrals
    print(paste(sol$minimum, pnorm(threshold, mean=PopMeans[i,j], PopSd),
                pnorm(threshold, mean=PopMeans[i,j], sqrt(PopSd^2+MeasSds[j]^2))))
  }
  print("")
}
```

```{r}
#| label: simulation
#| results: false
#| code-summary: "Code simulation"

if(!file.exists(paste0("SimresMatrix_",nrsimulations,"sim.rds"))){ # redo the simulation
  # do the simulation for n samples
  set.seed(1)
  dosim(nrsimulations)
  
  # write results to file
  res<- list(n=n, props=props, trainprop=trainprop, testprop=testprop, MeasSds=MeasSds, 
            PopSd=PopSd, threshold=threshold, nr_meas=nr_meas, 
            tedao=tedao, trdao=trdao, dsc=dsc, dsc2=dsc2)
  saveRDS(res, file=paste0("SimresMatrix_",nrsimulations,"sim.rds"))
} else {
  res<-readRDS(paste0("SimresMatrix_",nrsimulations,"sim.rds"))
}

dsc<-res$dsc
dsc2<-res$dsc2
tedao<-res$tedao
trdao<-res$trdao
props<-res$props
trainprop<-res$trainprop
testprop<-res$trainprop
MeasSds<-res$MeasSds
PopSd<-res$PopSd
threshold<-res$threshold
nr_meas<-res$nr_meas
# Calculate mean, sd, UL and LL AUPR/ROC values per combination of proportion of deferrals and measurement uncertainty
lp<-length(props)
lm<-length(MeasSds)
```

```{r}
#| label: fig-defrates
#| code-summary: "Code performance statistics and plot proportions"
#| fig-cap: The proportion of donors below the deferral threshold for the training and test data sets in the simulation as function of the preset proportion of deferral (1%, 2%, 4%, 8% and 16%).
#| fig-cap-location: margin

#auind: 1 for AUPR, 2 for ROC
#aucor: T for corrected area under the PR curve
#calcMedian: T for calculating median instead of mean
#plotResults: T for plotting AUPR/ROC results, F for plotting the deferral proportions of the samples
plotResultsorProportions(res, auind=1, aucor=T, calcMedian=F, plotResults =F, plotProportions=T, plotExchanged=F, plotboth=F)

```


## cAUPR

```{r}
#| label: fig-AUPR
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) over all simulations and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals.
#| fig-cap-location: margin
#| code-summary: "Code plot"

# plot AUPR results

plotResultsorProportions(res, auind=1, aucor=T, calcMedian=F, plotResults =T, plotProportions=F, plotExchanged=F, plotboth=F)

```

```{r}
#| label: fig-AUPRexchange
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets.
#| fig-cap-location: margin
#| code-summary: "Code plot"

# create plots
# plot main training results

plotResultsorProportions(res, auind=1, aucor=T, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=T, plotboth=F)
```

```{r}
#| label: tbl-cAUPR
#| code-summary: "Code table"
#| tbl-cap: "AUPR estimates and SD of Hb deferral prediction models developed on and applied to datasets with varying deferral rates and measurement errors."
#| tbl-cap-location: margin

aupr05 <- matrix(nrow=nrow(auMean1), ncol = ncol(auMean1)+ncol(auSd1))
for(i in 1:5){
  aupr05[1:5,(2*i)-1] <- auMean1[1:5,i] 
  aupr05[1:5,(2*i)] <- auSd1[1:5,i] 
}

aupr025 <- matrix(nrow=nrow(auMean2), ncol = ncol(auMean2)+ncol(auSd2))
for(i in 1:5){
  aupr025[,(2*i)-1] <- auMean2[,i] 
  aupr025[,(2*i)] <- auSd2[,i] 
}

rownames<- c("Training 1%", "Training 2%", "Training 4%", "Training 8%", "Training 16%")
colnames<- c("cAUPR", "Sd", "cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd")

auprtable <- rbind(aupr05, aupr025)
rownames(auprtable)<-c(rownames, rownames)
colnames(auprtable)<-colnames
kable(auprtable,
      digits=3,
      row.names=T
      )%>% add_header_above(c(" " = 1,"Test 1%"=2, "Test 2%"=2, "Test 4%"=2, "Test 8%"=2, "Test 16%"=2)) %>% pack_rows(index = c("Measurement error 0.5 mmol/L" = 5, "Measurement error 0.25 mmol/L" = 5))
```


```{r}
#| label: fig-AUPRbothmeaserr
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. In addition, cAUPR estimates for the prediction models are shown when applied to test datasets with lower measurement error (0.25 mmol/L).
#| fig-cap-location: margin
#| code-summary: "Code plot"
plotResultsorProportions(res, auind=1, aucor=T, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=T, plotboth=T)
```

## AUROC
```{r}
#| label: AUROC results
#| results: false
#| code-summary: "Code performance statistics (AUROC)"
plotResultsorProportions(res, auind=2, aucor=F, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=F, plotboth=F)

auroc05 <- matrix(nrow=nrow(auMean1), ncol = ncol(auMean1)+ncol(auSd1))
for(i in 1:5){
  auroc05[1:5,(2*i)-1] <- auMean1[1:5,i] 
  auroc05[1:5,(2*i)] <- auSd1[1:5,i] 
}
auroc025 <- matrix(nrow=nrow(auMean2), ncol = ncol(auMean2)+ncol(auSd2))
for(i in 1:5){
  auroc025[,(2*i)-1] <- auMean2[,i] 
  auroc025[,(2*i)] <- auSd2[,i] 
}
```

```{r}
#| label: fig-AUROC
#| fig-cap: Mean area under the ROC curve (AUROC) over all simulations and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| code-summary: "Code plot"
#| fig-cap-location: margin
plotResultsorProportions(res, auind=2, aucor=F, calcMedian=F, plotResults =T, plotProportions=F, plotExchanged=F, plotboth=F)
```

```{r}
#| label: fig-AUROCexchange
#| fig-cap: Mean area under the ROC curve (AUROC) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| code-summary: "Code plot"
#| fig-cap-location: margin

# create plots
# plot main training results
plotResultsorProportions(res, auind=2, aucor=F, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=T, plotboth=F)

```

```{r}
#| label: tbl-AUROC
#| code-summary: "Code table"
#| tbl-cap: "AUROC estimates and SD of Hb deferral prediction models developed on and applied to datasets with varying deferral rates and measurement errors."
#| tbl-cap-location: margin
rownames<- c("Training 1%", "Training 2%", "Training 4%", "Training 8%", "Training 16%")
colnames<- c("cAUPR", "Sd", "cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd")

auroctable <- rbind(auroc05, auroc025)
rownames(auroctable)<-c(rownames, rownames)
colnames(auroctable)<-colnames
kable(auroctable,
      digits=3,
      row.names=T
)%>% add_header_above(c(" " = 1,"Test 1%"=2, "Test 2%"=2, "Test 4%"=2, "Test 8%"=2, "Test 16%"=2)) %>% pack_rows(index = c("Measurement error 0.5 mmol/L" = 5, "Measurement error 0.25 mmol/L" = 5))
```


```{r}
#| label: fig-AUROCbothmeaserr
#| fig-cap: Mean area under the ROC curve (AUROC) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. In addition, AUROC estimates for the prediction models are shown when applied to test datasets with lower measurement error (0.25 mmol/L). For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| fig-cap-location: margin
#| code-summary: "Code plot"
plotResultsorProportions(res, auind=2, aucor=F, calcMedian=F, plotResults =F, plotProportions=F, plotExchanged=T, plotboth=T)

```
